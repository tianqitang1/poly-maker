"""
Market Scanner for Spike Momentum Bot - ENHANCED VERSION

Features:
1. Proper pagination handling (fetches ALL available markets)
2. sportsMarketType-based filtering (moneyline markets only)
3. Keyword-based fallback with word boundary matching
4. Filters out spread/over-under/total markets
"""

import asyncio
import json
import re
from typing import Dict, List, Any, Optional
import pandas as pd
from datetime import datetime
import aiohttp

from poly_data.polymarket_client import PolymarketClient
from spike_momentum.spike_detector import SpikeDetector, Spike
from spike_momentum.news_monitor import SportsNewsMonitor
from spike_momentum.llm_analyzer import LLMAnalyzer
from spike_momentum.post_res_arb import PostResolutionArbitrage
from poly_utils.logging_utils import get_logger

logger = get_logger('spike_momentum.scanner')


class MarketScanner:
    """Scans Polymarket for sports markets and monitors for spikes."""

    # Sports-specific keywords with word boundaries
    SPORTS_KEYWORDS = [
        # Leagues
        r'\bnfl\b', r'\bnba\b', r'\bmlb\b', r'\bnhl\b', r'\bmls\b',
        r'\bncaa\b', r'\bncaaf\b', r'\bncaab\b',
        r'premier league', r'champions league', r'\buefa\b', r'world cup', r'la liga',
        # Sports (word boundaries to avoid false matches)
        r'\bfootball\b', r'\bbasketball\b', r'\bbaseball\b', r'\bhockey\b', r'\bsoccer\b',
        # Events
        r'\bplayoff', r'super bowl', r'world series', r'stanley cup', r'\bfinals\b',
        r'championship', r'tournament', r'bowl game',
        # NFL Teams
        r'\bchiefs\b', r'\bbills\b', r'\bcowboys\b', r'\bpatriots\b',
        r'\b49ers\b', r'\beagles\b', r'\bpackers\b',
        r'\bravens\b', r'\bdolphins\b', r'\bbengals\b', r'\bbrowns\b',
        r'\bsteelers\b', r'\bcolts\b', r'\btexans\b',
        # NBA Teams
        r'\blakers\b', r'\bwarriors\b', r'\bceltics\b', r'\bheat\b',
        r'\bbucks\b', r'\bnuggets\b', r'\bsuns\b',
        r'\bmavericks\b', r'\bclippers\b', r'\bknicks\b',
        r'\bbulls\b', r'\bjazz\b', r'\bgrizzlies\b', r'\bhawks\b',
        # Soccer Teams
        r'\bmanchester\b', r'\bliverpool\b', r'\bbarcelona\b', r'real madrid',
        r'\bchelsea\b', r'\barsenal\b', r'\bbayern\b', r'\bpsg\b',
        # Player names (high-profile only)
        r'\bmahomes\b', r'\blebron\b', r'\bcurry\b', r'\bmessi\b', r'\bronaldo\b',
    ]

    # Category-based filtering (most reliable)
    SPORTS_CATEGORIES = ['sports', 'football', 'basketball', 'baseball', 'soccer', 'hockey']

    # Tags that indicate sports
    SPORTS_TAGS = ['sports', 'nfl', 'nba', 'mlb', 'nhl', 'soccer', 'football', 'basketball']

    def __init__(
        self,
        client: PolymarketClient,
        spike_detector: SpikeDetector,
        news_monitor: SportsNewsMonitor,
        llm_analyzer: Optional[LLMAnalyzer] = None,
        post_res_arb: Optional[PostResolutionArbitrage] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize market scanner.

        Args:
            client: Polymarket client
            spike_detector: Spike detector instance
            news_monitor: News monitor instance
            llm_analyzer: Optional LLM analyzer
            post_res_arb: Optional post-resolution arb detector
            config: Optional configuration
        """
        self.client = client
        self.spike_detector = spike_detector
        self.news_monitor = news_monitor
        self.llm_analyzer = llm_analyzer
        self.post_res_arb = post_res_arb
        self.config = config or {}

        # Market data
        self.sports_markets = {}  # market_id -> market_info
        self.market_questions = {}  # market_id -> question text
        self._live_poll_offset = 0  # round-robin cursor for normal priority polling batches
        self._started_poll_offset = 0 # round-robin cursor for started games polling batches

        # Compile regex patterns once
        self.keyword_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.SPORTS_KEYWORDS]

        strategies = []
        if spike_detector.enabled:
            strategies.append("spike_momentum")
        if post_res_arb and post_res_arb.enabled:
            strategies.append("post_res_arb")

        logger.info(f"Initialized MarketScanner with strategies: {strategies}")

    def fetch_sports_markets(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Fetch sports markets from Polymarket.

        Args:
            limit: Maximum markets to return (None = no limit, return all)

        Returns:
            List of market dictionaries
        """
        logger.info("Fetching markets from Polymarket...")

        try:
            # Try to fetch with tag filtering first (if API supports it)
            # This is much more efficient than fetching all markets
            sports_markets = self._fetch_markets_by_tags()

            if sports_markets:
                logger.info(f"Fetched {len(sports_markets)} markets using tag filtering")
            else:
                # Fallback: fetch all and filter manually
                logger.info("Tag filtering not available, fetching all markets...")
                sports_markets = self._fetch_all_markets_and_filter(limit)

            # Apply limit if specified
            markets_to_store = sports_markets if limit is None else sports_markets[:limit]

            # Store market info
            for market in markets_to_store:
                condition_id = market.get('condition_id', '')
                question = market.get('question', '')

                # Get tokens for this market
                tokens = market.get('tokens', [])
                if len(tokens) >= 2:
                    yes_token = tokens[0].get('token_id', '')
                    no_token = tokens[1].get('token_id', '')

                    # Slug used for live-game polling endpoint; fall back through common keys
                    market_slug = (
                        market.get('market_slug')
                        or market.get('slug')
                        or market.get('ticker')
                        or ''
                    )

                    # Extract game metadata from events (real-time game status!)
                    events = market.get('events', [])
                    game_metadata = {}
                    if events and len(events) > 0:
                        event = events[0]
                        game_metadata = {
                            'live': event.get('live', False),
                            'ended': event.get('ended', False),
                            'score': event.get('score', ''),
                            'period': event.get('period', ''),
                            'elapsed': event.get('elapsed', ''),
                            'finishedTimestamp': event.get('finishedTimestamp', ''),
                            'startTime': event.get('startTime', ''),
                        }

                        # Log game metadata samples for data analysis
                        # Log if: live game, ended game, or has score/period data
                        if event.get('live') or event.get('ended') or event.get('score') or event.get('period'):
                            logger.info(f"üéÆ GAME METADATA DETECTED [{condition_id[:8]}...]")
                            logger.info(f"   Market: {question[:80]}")
                            logger.info(f"   Event data: {json.dumps(event, indent=2)}")

                    self.sports_markets[condition_id] = {
                        'condition_id': condition_id,
                        'question': question,
                        'yes_token': yes_token,
                        'no_token': no_token,
                        'market_slug': market_slug,
                        'category': market.get('category', ''),
                        'end_date': market.get('end_date_iso', ''),
                        'game_start_time': market.get('gameStartTime', ''),  # When game actually starts
                        'game_metadata': game_metadata,  # Real-time game status!
                    }

                    self.market_questions[condition_id] = question

            logger.info(f"Stored {len(self.sports_markets)} sports markets")
            return markets_to_store

        except Exception as e:
            logger.error(f"Error fetching sports markets: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _fetch_markets_by_tags(self) -> List[Dict[str, Any]]:
        """Try to fetch markets using tag/category filtering."""
        sports_markets = []

        # Try different tags
        for tag in self.SPORTS_TAGS:
            try:
                # Note: This might not be supported by current API version
                # Check py-clob-client documentation
                response = self.client.client.get_markets(tag=tag)
                if response and isinstance(response, list):
                    sports_markets.extend(response)
                    logger.info(f"Found {len(response)} markets with tag '{tag}'")
            except TypeError:
                # API doesn't support tag parameter, return empty list
                return []
            except Exception as e:
                logger.debug(f"Error fetching by tag '{tag}': {e}")
                continue

        return sports_markets

    def _fetch_all_markets_and_filter(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Fallback: fetch all markets and filter manually with proper pagination."""
        cursor = ""
        all_markets = []
        page = 0
        max_iterations = 50  # Safety limit to prevent infinite loops

        # Fetch ALL markets using pagination (like update_markets.py does)
        while page < max_iterations:
            try:
                response = self.client.client.get_sampling_markets(next_cursor=cursor)
                markets_data = response.get('data', [])

                if not markets_data:
                    break

                all_markets.extend(markets_data)
                page += 1

                # Progress logging (like near_sure does)
                logger.info(f"Fetched page {page}: {len(markets_data)} markets (Total: {len(all_markets)})")

                cursor = response.get('next_cursor')
                if cursor is None:
                    logger.info(f"Pagination complete: {page} pages fetched")
                    break

            except Exception as e:
                # API sometimes fails with bad cursor after exhausting markets
                # This is expected behavior at the end of pagination
                logger.info(f"Pagination ended after {page} pages (fetched {len(all_markets)} markets): {e}")
                break

        logger.info(f"Fetched {len(all_markets)} total markets, now filtering for sports...")

        # Filter for sports markets (moneyline only)
        sports_markets = []
        moneyline_count = 0
        filtered_count = 0

        # Progress tracking for filtering (show every 500 markets)
        processed = 0
        for market in all_markets:
            processed += 1
            if processed % 500 == 0:
                logger.info(f"Filtering progress: {processed}/{len(all_markets)} markets checked...")

            # Check for sportsMarketType at market level (not in events!)
            sports_type = market.get('sportsMarketType')
            if sports_type:
                if sports_type.lower() == 'moneyline':
                    moneyline_count += 1
                else:
                    filtered_count += 1

            if self._is_sports_market(market):
                sports_markets.append(market)

        logger.info(f"Found {len(sports_markets)} sports markets after filtering")
        if moneyline_count > 0:
            logger.info(f"  ‚îú‚îÄ {moneyline_count} moneyline markets (via sportsMarketType)")
        if filtered_count > 0:
            logger.info(f"  ‚îú‚îÄ {filtered_count} non-moneyline sports markets (filtered out)")
        if len(sports_markets) > moneyline_count:
            logger.info(f"  ‚îî‚îÄ {len(sports_markets) - moneyline_count} markets (via keyword matching)")

        return sports_markets

    def _is_sports_market(self, market: Dict[str, Any]) -> bool:
        """
        Check if market is sports-related using sportsMarketType (preferred) or keywords.

        Only accepts moneyline markets for simplicity.
        """
        # PRIORITY 1: Check for sportsMarketType field at market level (most reliable!)
        # This field indicates real sports markets and their type
        sports_market_type = market.get('sportsMarketType')

        if sports_market_type:
            # Only trade moneyline markets (most straightforward)
            # Skip debug logging to speed up filtering (called ~3000 times)
            return sports_market_type.lower() == 'moneyline'

        # FALLBACK: Use keyword-based detection if sportsMarketType not available
        question = market.get('question', '').lower()
        category = market.get('category', '').lower()
        tags = market.get('tags', [])

        # Early check: avoid spread/over-under markets
        spread_keywords = ['spread', 'over ', 'under ', 'o/u', 'total points']
        is_spread_market = any(keyword in question for keyword in spread_keywords)

        # Check category (most reliable fallback)
        if category in self.SPORTS_CATEGORIES:
            return not is_spread_market

        # Check tags
        if tags and any(tag.lower() in self.SPORTS_TAGS for tag in tags):
            return not is_spread_market

        # Check question with word boundary patterns
        if any(pattern.search(question) for pattern in self.keyword_patterns):
            return not is_spread_market

        return False

    def get_token_ids(self) -> List[str]:
        """Get list of token IDs to subscribe to."""
        token_ids = []

        for market_info in self.sports_markets.values():
            yes_token = market_info.get('yes_token')
            no_token = market_info.get('no_token')

            if yes_token:
                token_ids.append(yes_token)
            if no_token:
                token_ids.append(no_token)

        logger.info(f"Generated {len(token_ids)} token IDs to monitor")
        return token_ids

    async def _fetch_live_game_status(self, market_slug: str) -> Optional[Dict[str, Any]]:
        """
        Fetch live game status from Gamma API for a specific market.

        Args:
            market_slug: The market slug identifier

        Returns:
            Game metadata dict with live, score, period, etc. or None if error
        """
        try:
            url = f"https://gamma-api.polymarket.com/markets/slug/{market_slug}"

            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status != 200:
                        logger.debug(f"Failed to fetch {market_slug}: HTTP {response.status}")
                        return None

                    data = await response.json()

                    # Extract events data (same structure as manual curl)
                    events = data.get('events', [])
                    if not events or len(events) == 0:
                        logger.debug(f"No events array for {market_slug}")
                        # Return empty metadata to indicate "not a live game market"
                        return {
                            'live': False,
                            'ended': False,
                            'score': '',
                            'period': '',
                            'elapsed': '',
                            'startTime': '',
                            'finishedTimestamp': '',
                        }

                    event = events[0]
                    game_metadata = {
                        'live': event.get('live', False),
                        'ended': event.get('ended', False),
                        'score': event.get('score', ''),
                        'period': event.get('period', ''),
                        'elapsed': event.get('elapsed', ''),
                        'startTime': event.get('startTime', ''),
                        'finishedTimestamp': event.get('finishedTimestamp', ''),
                    }

                    # Log if we found a live or ended game
                    if game_metadata.get('live') or game_metadata.get('ended'):
                        logger.info(f"Found {'LIVE' if game_metadata.get('live') else 'ENDED'} game: {market_slug}")

                    return game_metadata

        except asyncio.TimeoutError:
            logger.debug(f"Timeout fetching live status for {market_slug}")
            return None
        except Exception as e:
            logger.debug(f"Error fetching live status for {market_slug}: {e}")
            return None

    async def _update_live_games_loop(self):
        """
        Background task that periodically updates live game status for all markets.
        Runs every 30 seconds to check for live games.
        """
        logger.info("Starting live game status updater...")

        while True:
            try:
                # Wait 30 seconds between updates
                await asyncio.sleep(30)

                # Find markets that might be live (have game_start_time in the past)
                now = datetime.utcnow()

                # Build prioritized list of candidates
                actually_live = []    # Priority 0: Metadata says LIVE (check every loop!)
                started_in_past = []  # Priority 1: Time says started (check frequently, rotated)
                normal_priority = []  # Priority 2: Future games (check occasionally, rotated)

                for condition_id, market_info in self.sports_markets.items():
                    # Skip if already ended AND finished > 12 hours ago
                    # We want to keep checking recently ended games for arb opportunities
                    game_metadata = market_info.get('game_metadata', {})
                    if game_metadata.get('ended'):
                        should_skip = False
                        finished_ts = game_metadata.get('finishedTimestamp')
                        
                        if finished_ts:
                            try:
                                from dateutil import parser
                                finished_time = parser.parse(finished_ts).replace(tzinfo=None)
                                hours_since_finish = (now - finished_time).total_seconds() / 3600
                                
                                # specific check: if > 12 hours, we're done with it
                                if hours_since_finish > 12:
                                    should_skip = True
                            except Exception:
                                # If we can't parse timestamp, don't skip (safety)
                                pass
                        
                        if should_skip:
                            continue

                    market_slug = market_info.get('market_slug')
                    if not market_slug:
                        continue
                        
                    # Check if game is actually live per metadata
                    if game_metadata.get('live'):
                        actually_live.append((condition_id, market_slug))
                        continue

                    # Check if game has started (using gameStartTime)
                    game_start_time_str = market_info.get('game_start_time', '')
                    is_started = False

                    if game_start_time_str:
                        try:
                            from dateutil import parser
                            game_start_time = parser.parse(game_start_time_str).replace(tzinfo=None)
                            time_since_start = (now - game_start_time).total_seconds()

                            # Game started in the past
                            if time_since_start > 0:
                                is_started = True
                        except Exception:
                            pass

                    if is_started:
                        started_in_past.append((condition_id, market_slug))
                    else:
                        normal_priority.append((condition_id, market_slug))

                # Combine: 
                # 1. All actually_live games (limit to batch_size)
                # 2. Rotate through started_in_past
                # 3. Rotate through normal_priority
                
                batch_size = 60  # Increased from 20 to 60 (2 req/s) for better coverage
                candidates_to_check = []

                # 1. Actually Live (Take ALL up to batch limit)
                candidates_to_check.extend(actually_live[:batch_size])
                
                remaining_slots = batch_size - len(candidates_to_check)

                # 2. Started in Past (Round Robin)
                if remaining_slots > 0 and started_in_past:
                    pool = started_in_past
                    count = len(pool)
                    offset = self._started_poll_offset
                    
                    # Determine how many to take
                    # We want to prioritize these over normal priority
                    # But we also want to leave a little room for normal priority if possible? 
                    # No, started games are much more important for arb. 
                    # Take as many as possible.
                    
                    take_count = min(remaining_slots, count)
                    
                    start = offset % count
                    
                    batch = pool[start : start + take_count]
                    
                    # Wrap around
                    if len(batch) < take_count:
                        wrap_needed = take_count - len(batch)
                        batch.extend(pool[:wrap_needed])
                        
                    candidates_to_check.extend(batch)
                    self._started_poll_offset = (start + take_count) % count
                    
                    remaining_slots -= len(batch)

                # 3. Normal Priority (Round Robin)
                if remaining_slots > 0 and normal_priority:
                    pool = normal_priority
                    count = len(pool)
                    offset = self._live_poll_offset
                    
                    take_count = min(remaining_slots, count)
                    
                    start = offset % count
                    
                    batch = pool[start : start + take_count]
                    
                    # Wrap around
                    if len(batch) < take_count:
                        wrap_needed = take_count - len(batch)
                        batch.extend(pool[:wrap_needed])
                        
                    candidates_to_check.extend(batch)
                    self._live_poll_offset = (start + take_count) % count

                logger.debug(f"Live candidates: {len(actually_live)} actual, {len(started_in_past)} started, {len(normal_priority)} normal")
                logger.info(f"Checking {len(candidates_to_check)} markets for live game status (Live: {len(actually_live)}, Started: {len(started_in_past)})")

                live_count = 0
                ended_count = 0
                checked_count = 0

                for condition_id, market_slug in candidates_to_check:
                    checked_count += 1
                    game_metadata = await self._fetch_live_game_status(market_slug)

                    if game_metadata:
                        # Update market metadata
                        self.sports_markets[condition_id]['game_metadata'] = game_metadata

                        if game_metadata.get('live'):
                            live_count += 1
                            logger.debug(
                                f"Live game: {self.sports_markets[condition_id]['question'][:50]} - "
                                f"Score: {game_metadata.get('score', 'N/A')}, "
                                f"Period: {game_metadata.get('period', 'N/A')}"
                            )

                        if game_metadata.get('ended'):
                            ended_count += 1
                            logger.info(
                                f"üèÅ Game ended: {self.sports_markets[condition_id]['question'][:50]} - "
                                f"Final score: {game_metadata.get('score', 'N/A')}"
                            )

                        # Check for post-resolution arb on live or ended games
                        if (game_metadata.get('live') or game_metadata.get('ended')) and self.post_res_arb:
                            if self.post_res_arb.enabled:
                                # Fetch current order book to get price
                                try:
                                    market_info = self.sports_markets[condition_id]
                                    yes_token = market_info.get('yes_token')
                                    no_token = market_info.get('no_token')

                                    if yes_token and no_token:
                                        book = {}
                                        try:
                                            if hasattr(self.client, 'get_order_book_dict'):
                                                book = self.client.get_order_book_dict(yes_token)
                                            else:
                                                book = self.client.get_order_book(yes_token)
                                        except Exception as e:
                                            logger.warning(f"Order book fetch failed for {yes_token}: {e}")

                                        bids = book.get('bids', []) if isinstance(book, dict) else []
                                        asks = book.get('asks', []) if isinstance(book, dict) else []

                                        # Legacy tuple/DataFrame support
                                        if not bids and not asks and isinstance(book, tuple) and len(book) == 2:
                                            bids_df, asks_df = book
                                            try:
                                                bids = bids_df.to_dict('records') if not bids_df.empty else []
                                                asks = asks_df.to_dict('records') if not asks_df.empty else []
                                            except Exception:
                                                bids, asks = [], []

                                        if bids and asks:
                                            best_bid = float(bids[0]['price']) if bids else None
                                            best_ask = float(asks[0]['price']) if asks else None

                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2

                                                arb_opp = await self.post_res_arb.check_market_for_arb(
                                                    market_id=condition_id,
                                                    market_question=market_info['question'],
                                                    current_price=mid_price,
                                                    market_info=market_info
                                                )

                                                if arb_opp:
                                                    await self._handle_post_res_arb(arb_opp)
                                except Exception as e:
                                    logger.error(f"Error checking post-res arb for {condition_id[:8]}...: {e}")

                logger.info(f"Live game polling: checked {checked_count} markets, found {live_count} live, {ended_count} ended")

                if live_count == 0 and ended_count == 0 and checked_count > 0:
                    logger.debug(f"No live or ended games found in {checked_count} markets checked")

            except Exception as e:
                logger.error(f"Error in live game updater: {e}")
                import traceback
                traceback.print_exc()

    async def monitor_markets(self):
        """
        Monitor markets via WebSocket and detect spikes.

        This is the main monitoring loop with automatic reconnection.
        """
        logger.info("Starting market monitoring...")

        # Fetch all sports markets (no limit - monitor everything we find)
        sports_markets = self.fetch_sports_markets(limit=None)

        if not sports_markets:
            logger.error("No sports markets found to monitor")
            return

        # Get token IDs to subscribe to
        token_ids = self.get_token_ids()

        if not token_ids:
            logger.error("No token IDs to monitor")
            return

        print(f"\n‚úì Monitoring {len(self.sports_markets)} sports markets ({len(token_ids)} tokens)")
        print("\nSample markets:")
        for i, (cid, info) in enumerate(list(self.sports_markets.items())[:5]):
            print(f"  {i+1}. {info['question'][:80]}")
        print()

        # Start background task for live game updates
        live_game_task = asyncio.create_task(self._update_live_games_loop())
        logger.info("Started live game status monitoring")

        # Reconnection loop with exponential backoff
        reconnect_delay = 1  # Start with 1 second
        max_reconnect_delay = 60  # Max 60 seconds

        while True:
            try:
                await self._connect_and_monitor(token_ids)
                # If we get here, connection closed gracefully - reset delay
                reconnect_delay = 1

            except Exception as e:
                logger.error(f"WebSocket connection error: {e}")
                logger.info(f"Reconnecting in {reconnect_delay} seconds...")
                await asyncio.sleep(reconnect_delay)

                # Exponential backoff
                reconnect_delay = min(reconnect_delay * 2, max_reconnect_delay)

    async def _connect_and_monitor(self, token_ids: List[str]):
        """Connect to WebSocket and monitor until connection drops."""
        import websockets
        from websockets.exceptions import ConnectionClosed, WebSocketException

        uri = "wss://ws-subscriptions-clob.polymarket.com/ws/market"

        async with websockets.connect(uri, ping_interval=20, ping_timeout=10) as websocket:
            # Subscribe to markets
            message = {"assets_ids": token_ids}
            await websocket.send(json.dumps(message))

            logger.info(f"Subscribed to {len(token_ids)} token IDs")
            print(f"‚úì Connected to WebSocket\n")
            print("Waiting for price spikes...\n")

            # Process incoming messages
            while True:
                try:
                    message = await websocket.recv()
                    json_data = json.loads(message)

                    # Process market updates
                    if isinstance(json_data, dict):
                        await self._process_market_update([json_data])
                    elif isinstance(json_data, list):
                        await self._process_market_update(json_data)

                except (ConnectionClosed, WebSocketException) as e:
                    # WebSocket connection lost - raise to trigger reconnection
                    logger.warning(f"WebSocket connection closed: {e}")
                    raise

                except json.JSONDecodeError as e:
                    # Invalid JSON - log and continue
                    logger.warning(f"Invalid JSON received: {e}")
                    continue

                except Exception as e:
                    # Other errors - log and continue
                    logger.error(f"Error processing message: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

    async def _process_market_update(self, updates: List[Dict[str, Any]]):
        """Process market updates from WebSocket."""
        for update in updates:
            event_type = update.get('event_type')

            if event_type not in ['book', 'price_change']:
                continue

            # Get market identifier
            market = update.get('market', '')
            asset_id = update.get('asset_id', '')

            # Find condition_id from market or asset_id
            condition_id = None
            for cid, info in self.sports_markets.items():
                if info.get('yes_token') == asset_id or info.get('no_token') == asset_id:
                    condition_id = cid
                    break

            if not condition_id:
                continue

            # Get market question
            question = self.market_questions.get(condition_id, 'Unknown market')

            # Extract price data
            if event_type == 'book':
                # Full order book
                bids = update.get('bids', [])
                asks = update.get('asks', [])

                if bids and asks:
                    best_bid = float(bids[0]['price']) if bids else 0.0
                    best_ask = float(asks[0]['price']) if asks else 0.0
                    mid_price = (best_bid + best_ask) / 2

                    # Check for spike momentum opportunity
                    if self.spike_detector.enabled:
                        spike = self.spike_detector.update_price(
                            market_id=condition_id,
                            market_question=question,
                            best_bid=best_bid,
                            best_ask=best_ask
                        )

                        if spike:
                            await self._handle_spike(spike)

                    # Check for post-resolution arb opportunity
                    if self.post_res_arb and self.post_res_arb.enabled:
                        market_info = self.sports_markets.get(condition_id, {})
                        arb_opp = await self.post_res_arb.check_market_for_arb(
                            market_id=condition_id,
                            market_question=question,
                            current_price=mid_price,
                            market_info=market_info
                        )

                        if arb_opp:
                            await self._handle_post_res_arb(arb_opp)

            elif event_type == 'price_change':
                # Price change update - could implement incremental updates here
                pass

    async def _handle_spike(self, spike: Spike):
        """Handle detected spike."""
        logger.info(f"Spike detected: {spike}")

        print(f"\n{'='*120}")
        print(f"üö® SPIKE DETECTED!")
        print(f"{'='*120}")
        print(f"Market: {spike.market_question}")
        print(f"Direction: {spike.direction.upper()}")
        print(f"Price change: {spike.price_change_pct:+.2f}% ({spike.previous_price:.3f} ‚Üí {spike.current_price:.3f})")
        print(f"Time window: {spike.time_window}s")
        print(f"Spike strength: {spike.spike_strength:.2f} std devs")

        # Display live game status if available
        market_info = self.sports_markets.get(spike.market_id)
        if market_info:
            game_metadata = market_info.get('game_metadata', {})
            if game_metadata and (game_metadata.get('live') or game_metadata.get('score')):
                print(f"\nüèüÔ∏è  Live Game Status:")
                if game_metadata.get('live'):
                    print(f"  Status: üî¥ LIVE")
                elif game_metadata.get('ended'):
                    print(f"  Status: ‚úÖ ENDED")
                else:
                    print(f"  Status: Pre-game")

                if game_metadata.get('score'):
                    print(f"  Score: {game_metadata['score']}")
                if game_metadata.get('period'):
                    print(f"  Period: {game_metadata['period']}")
                if game_metadata.get('elapsed'):
                    print(f"  Time: {game_metadata['elapsed']}")

        # Find relevant news
        news_matches = self.news_monitor.match_to_market(spike.market_question, max_results=3)

        if news_matches:
            print(f"\nRelated news ({len(news_matches)} items):")
            for match in news_matches:
                item = match['news']
                print(f"  [{item.source}] {item.title}")
                print(f"    Relevance: {match['relevance_score']:.2f}")
        else:
            print(f"\nNo related news found")

        # LLM analysis (if available) - runs even without news if we have game data
        if self.llm_analyzer:
            # Only run LLM if we have game metadata OR news
            if game_metadata or news_matches:
                print(f"\nRunning LLM analysis...")
                analysis = self.llm_analyzer.analyze_spike(
                    market_question=spike.market_question,
                    current_price=spike.current_price,
                    previous_price=spike.previous_price,
                    price_change_pct=spike.price_change_pct,
                    game_metadata=game_metadata,
                    news_items=[m['news'] for m in news_matches] if news_matches else None
                )

                if analysis:
                    print(f"\nüìä LLM Analysis:")
                    print(f"  Justified: {'YES' if analysis.justified else 'NO'}")
                    print(f"  Confidence: {analysis.confidence}%")
                    print(f"  Recommendation: {analysis.recommendation.upper()}")
                    print(f"  Reasoning: {analysis.reasoning}")

                    if analysis.near_resolution:
                        print(f"  ‚è∞ Near resolution: {analysis.estimated_time_to_resolution}")

                    should_trade = self.llm_analyzer.should_trade(analysis)
                    print(f"\n  {'‚úÖ WOULD TRADE' if should_trade else '‚ùå SKIP'} (in live mode)")

        print(f"{'='*120}\n")

    async def _handle_post_res_arb(self, opportunity: Dict[str, Any]):
        """Handle post-resolution arb opportunity."""
        logger.info(f"Post-res arb opportunity: {opportunity}")

        # Execute arb (dry-run for now)
        dry_run = self.config.get('operation', {}).get('dry_run', True)
        result = await self.post_res_arb.execute_arb(opportunity, dry_run=dry_run)

        if result.get('success'):
            logger.info(f"Arb executed successfully: {result}")
        else:
            logger.warning(f"Arb execution failed: {result.get('error')}")


async def run_live_scanner(config_path: str = 'spike_momentum/config.yaml'):
    """Run the live market scanner."""
    import yaml
    from spike_momentum.llm_provider import LLMProvider

    # Load config
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # Initialize Polymarket client (read-only, no trading)
    try:
        client = PolymarketClient()
    except ValueError as e:
        print(f"‚ö†Ô∏è  No Polymarket credentials: {e}")
        print("   Some features may be limited")
        return

    # Initialize components
    news_monitor = SportsNewsMonitor(config.get('news', {}))
    spike_detector = SpikeDetector(config.get('spike_detection', {}))

    # LLM provider (shared across strategies)
    llm_provider = None
    if config.get('llm', {}).get('enabled', True):
        try:
            llm_provider = LLMProvider(config['llm'])
            print(f"‚úì LLM provider initialized ({config['llm']['provider']})\n")
        except Exception as e:
            logger.warning(f"LLM provider disabled: {e}")
            print(f"‚ö†Ô∏è  LLM provider disabled: {e}\n")

    # LLM analyzer (for spike momentum)
    llm_analyzer = None
    if llm_provider:
        llm_analyzer = LLMAnalyzer(llm_provider, config['llm'])

    # Post-resolution arb (separate strategy)
    post_res_arb = None
    if config.get('post_resolution_arb', {}).get('enabled', False):
        post_res_arb = PostResolutionArbitrage(
            client=client,
            news_monitor=news_monitor,
            llm_provider=llm_provider,
            config=config
        )
        print(f"‚úì Post-resolution arb initialized\n")

    # Display capital allocation
    capital_config = config.get('capital', {})
    if capital_config:
        total = capital_config.get('total_capital', 0)
        spike_pct = capital_config.get('spike_momentum', 0) * 100
        arb_pct = capital_config.get('post_resolution_arb', 0) * 100
        print(f"Capital Allocation (${total:.0f} total):")
        print(f"  Spike Momentum: {spike_pct:.0f}% (${total * spike_pct/100:.0f})")
        print(f"  Post-Res Arb: {arb_pct:.0f}% (${total * arb_pct/100:.0f})\n")

    # Initialize scanner
    scanner = MarketScanner(
        client=client,
        spike_detector=spike_detector,
        news_monitor=news_monitor,
        llm_analyzer=llm_analyzer,
        post_res_arb=post_res_arb,
        config=config
    )

    # Fetch initial news
    print("Fetching initial news...")
    news_items = news_monitor.fetch_news(max_items=20)
    print(f"‚úì Loaded {len(news_items)} news items\n")

    # Start monitoring
    await scanner.monitor_markets()


if __name__ == '__main__':
    asyncio.run(run_live_scanner())
